{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W2 - Laboratory.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lH-vP8rGhXMC","colab_type":"text"},"source":["# Fast Forward into Word Embeddings - Laboratory\n","**Module:**\n","1. Playing around with classical embedding models\n","2. Playing around with deep embedding models\n","3. Playing around with simple information retrieval mechanism"]},{"cell_type":"code","metadata":{"id":"_pkafwGYiDTp","colab_type":"code","colab":{}},"source":["!pip install glove_python"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCzMpfVLhXMD","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('punkt')\n","\n","from bs4 import BeautifulSoup\n","from gensim.models import Word2Vec\n","from glove import Corpus, Glove\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","from nltk.tokenize import sent_tokenize\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n","import string\n","from tqdm import tqdm\n","tqdm.pandas()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kn-tCiDlhXMI","colab_type":"text"},"source":["## 0. Preparing and Preprocessing the Dataset\n","We will use the same dataset from W1 Lab. we can use preprocessing method you learned in W1 here."]},{"cell_type":"code","metadata":{"id":"b4rQusIZiyve","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmyO3JyahXMJ","colab_type":"code","colab":{}},"source":["data = pd.read_csv('DATASET_PATH')\n","data[['Text']].head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tswdk_E3hXMQ","colab_type":"code","colab":{}},"source":["print data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UPltEdZhXMV","colab_type":"code","colab":{}},"source":["# preprocess the text\n","def remove_punctuations(text):\n","    return text.translate(string.maketrans('', ''), string.punctuation)\n","def strip_html(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","def preprocess(text):\n","    text = strip_html(text)\n","    text = text.encode('utf-8')\n","    text = remove_punctuations(text)\n","    return text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"R3EW8uYHhXMY","colab_type":"code","colab":{}},"source":["data['text_tfidf'] = data['Text'].progress_apply(preprocess)\n","data[['text_tfidf']].head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5unsqXghXMc","colab_type":"text"},"source":["## 1. Playing around with classical embedding models\n","We will use sklearn for this module. There are 3 different classes in sklearn which we can use:\n","- CountVectorizer,\n","- HashingVectorizer, and\n","- TfidfVectorizer."]},{"cell_type":"markdown","metadata":{"id":"U0UzvCzthXMd","colab_type":"text"},"source":["### Task 1.1: Transform each document into vectors using TfidfVectorizer\n","TfidfVectorizer is very easy to use. It also contains many different functionalities, such as preprocessing (e.g. lower_case, stop_words) and weighting parameters.\n","**Task**: Let's try to use the default setting to build the embeddings, i.e. *raw_count* tf and *smooth_idf*."]},{"cell_type":"code","metadata":{"id":"hlMN0IE8hXMe","colab_type":"code","colab":{}},"source":["tfidf_model = TfidfVectorizer(\n","                    preprocessor=None, # if we need custom preprocessor\n","                    tokenizer=None, # if we need custom tokenizer\n","                    stop_words='english', # we can pass list as well\n","                    max_features=1000, # number of vocabularies, or we can set the vocabulary\n","                    norm=None, # l2 is useful for cosine_distance\n","                    binary=False, # if true then tf will be binary\n","                    sublinear_tf=False, # if true then tf will be log_normalization\n","                    use_idf=True, # if false then idf will be unary\n","                    smooth_idf=True # if true then idf will be idf_smooth\n","                )\n","tfidf_model.fit(data.text_tfidf.values) # we can also use fit_transform to get the result directly"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxkunO_NhXMj","colab_type":"code","colab":{}},"source":["# useful properties\n","# print tfidf_model.get_stop_words() # get the stopwords\n","# print tfidf_model.idf_ # get the idf of each word\n","# print tfidf_model.vocabulary_ # get the vocabularies"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Yv8w1sZhXMm","colab_type":"code","colab":{}},"source":["print data.at[0,'text_tfidf']\n","print tfidf_model.transform([data.at[0,'text_tfidf']])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"caal7wzVhXMq","colab_type":"code","colab":{}},"source":["text = 'I love dog'\n","print text\n","print tfidf_model.transform([text])\n","# print tfidf_model.transform([text]).toarray() # to convert sparse matrix into array"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9qNh-TA_hXMz","colab_type":"text"},"source":["### Task 1.2: Playing around with TfidfVectorizer\n","We have tried the default setting of TfidfVectorizer. **Task**: Let's try using *binary_tf* and/or *unary_idf*."]},{"cell_type":"code","metadata":{"id":"S96Pj6ZqhXM0","colab_type":"code","colab":{}},"source":["# code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyEkgfcRhXM3","colab_type":"text"},"source":["### Task 1.3: Transform each document into vectors using CountVectorizer and HashingVectorizer.\n","CountVectorizer and HashingVectorizer are very similar with TfidfVectorizer in terms of usage. One of the difference is that they don't use idf to build the embeddings. However, they are much faster than TfidfVectorizer (well, I guess so).\n","**Task**: Let's play around with CountVectorizer and HashingVectorizer."]},{"cell_type":"code","metadata":{"id":"rm1XZFa9hXM4","colab_type":"code","colab":{}},"source":["# code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOJJ8pfMhXM7","colab_type":"text"},"source":["## 2. Playing around with deep embedding models\n","We will use gensim and glove for this module. We will create word embeddings using Word2Vec and Glove."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5Y-gm1sThXM8","colab_type":"code","colab":{}},"source":["# preprocess the data\n","sentences = [sent_tokenize(text.decode('utf-8')) for text in tqdm(data.Text.values)]\n","sentences = [preprocess(stc).split() for stcs in tqdm(sentences) for stc in stcs]\n","print len(sentences)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6kB2nKuhXNB","colab_type":"text"},"source":["### Task 2.1: Build word embeddings using Word2Vec\n","Using the same dataset, we would like to build our word embeddings. Note that our dataset is actually very few. Here, we just focus on building the word embeddings. **Task**: Let's build SG word embeddings with 128 word dimension."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_CwlH_xwhXNC","colab_type":"code","colab":{}},"source":["w2v_model = Word2Vec(\n","                    sentences=sentences, \n","                    size=128, # number of vector dimension\n","                    window=5, # maximum distance between the current and predicted words\n","                    min_count=2, # will remove words with occurrence less than this value\n","                    max_vocab_size=None, # limit the number of words\n","                    sg=1, # if 1, then skipgram is used; if 0, then cbow\n","                    negative=20, # number of random negative sampling\n","                    iter=10, # number of iteration to the whole document\n","                    workers=-1 # number of cpu cores to use\n","                )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nEMmkbrhXNG","colab_type":"code","colab":{}},"source":["# useful properties\n","# print w2v_model.wv['dog'] # get the word vector of 'dog'\n","# print w2v_model.wv.index2word # get the vocabularies\n","# print w2v_model.wv.vectors # get all word vectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iHjuUc_UhXNK","colab_type":"code","colab":{}},"source":["# get most similar words\n","w2v_model.wv.most_similar('store')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7EJZWDrhXNQ","colab_type":"text"},"source":["### Task 2.1: Build word embeddings using Glove\n","Task: Let's build glove word embeddings with 128 word dimension."]},{"cell_type":"code","metadata":{"id":"Yfgv48O6hXNR","colab_type":"code","colab":{}},"source":["corpus = Corpus()\n","corpus.fit(sentences, window=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oviO0FlUhXNX","colab_type":"code","colab":{}},"source":["glove = Glove(\n","            no_components=128, # number of vector dimension\n","            learning_rate=0.05,\n","            alpha=0.75, # weighting\n","            max_count=100 # weighting\n","        )\n","glove.fit(corpus.matrix, epochs=10, no_threads=4, verbose=True)\n","glove.add_dictionary(corpus.dictionary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCTtnxBvhXNc","colab_type":"code","colab":{}},"source":["# useful properties\n","# print glove.dictionary\n","# print glove.word_biases\n","# print glove.word_vectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3h0N7XFhXNe","colab_type":"code","colab":{}},"source":["# get most similar words\n","glove.most_similar('store', 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HY2JIPJRhXNi","colab_type":"text"},"source":["## 3. Playing around with simple information retrieval mechanism\n","After we successfully transformed every document into vector, we would like to build simple information retrieval algorithm using *cosine similarity*. We will do matrix calculation using numpy."]},{"cell_type":"markdown","metadata":{"id":"z_k2_sXDhXNj","colab_type":"text"},"source":["### Task 3.1: Find most similar documents using tfidf vectors\n","We have built our tfidf_model in Task 1.1. However, it's not normalized yet. To make the calculation easier, we need to normalize each vector since we are going to use cosine similarity.\n","**Task**: Let's normalize each tfidf document vector and build simple document search."]},{"cell_type":"code","metadata":{"id":"n1YzP8j1hXNk","colab_type":"code","colab":{}},"source":["# transform documents into tfidf vectors\n","tfidf_vectors = tfidf_model.transform(data.Text.values[:10000]).toarray()\n","print tfidf_vectors.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Imx0vwdlhXNn","colab_type":"code","colab":{}},"source":["# normalize the vector using l2 norm to each vector length is 1\n","tfidf_vectors /= np.linalg.norm(tfidf_vectors, axis=1).reshape(-1,1) + 1e-10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNqnui0zhXNq","colab_type":"code","colab":{}},"source":["def retrieve_documents(text, k=5):\n","    # return top_k most similar documents\n","    print 'query text: {}'.format(text)\n","    print\n","    \n","    query_vector = tfidf_model.transform([text]).toarray()\n","    query_vector /= np.linalg.norm(query_vector, axis=1).reshape(-1,1) + 1e-10\n","    \n","    scores = np.sum(tfidf_vectors*query_vector, axis=1)\n","    results = zip(np.sort(scores)[::-1], data.Text.values[np.argsort(scores)[::-1]])\n","    \n","    for i in range(k):\n","        print 'result {}: {}'.format(i+1, results[i][0])\n","        print results[i][1]\n","        print "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6sMnMMlhXNt","colab_type":"code","colab":{}},"source":["retrieve_documents('I love dog')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VBbNUaQ-hXN0","colab_type":"code","colab":{}},"source":["retrieve_documents(data.at[15000,'Text'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOwzW_TkhXN5","colab_type":"text"},"source":["### Task 3.2: Find most similar documents using Word2Vec & Glove vectors\n","We have built our word2vec_model in Task 2.1. We obtained the word embeddings. However, we would like to compare the document similarity, not word similarity. The easiest method is to just average all the word vectors in the document to get the document vector. **Task**: Let's use the word2vec embedding to find the most similar documents."]},{"cell_type":"code","metadata":{"id":"zbiYtIkDhXN7","colab_type":"code","colab":{}},"source":["# code here"],"execution_count":0,"outputs":[]}]}