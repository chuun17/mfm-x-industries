{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W1 - Laboratory.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"q6WUI_oycvWU","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","\n","# for the first time, should download these dependencies\n","# nltk.download('stopwords')\n","# nltk.download('punkt')\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNAbKfHX1yX-","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lONF8kkScvWi","colab_type":"text"},"source":["# Load the Dataset"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Of5o1FuvcvWl","colab_type":"code","colab":{}},"source":["df_review = pd.read_csv('DATASET_PATH', engine='python')\n","df_review.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIzHv2CRywZ_","colab_type":"code","colab":{}},"source":["len(df_review)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"finlJUl5cvWs","colab_type":"text"},"source":["# Noise Removal"]},{"cell_type":"code","metadata":{"id":"bXgrQhG_cvWu","colab_type":"code","colab":{}},"source":["# remove any html tag\n","def strip_html(text):\n","    from bs4 import BeautifulSoup\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","# replacing contractions with their expansions\n","def replace_contractions(text):\n","    import contractions\n","    \"\"\"Replace contractions in string of text\"\"\"\n","    return contractions.fix(text)\n","\n","# remove punctuation\n","# ...\n","# HINT:\n","# - translate function\n","\n","# replace email\n","# ...\n","# HINT:\n","# - regex sub\n","\n","# spelling correction\n","# ...\n","# HINT:\n","# - create an instance of TextBlob from the text\n","# - invoke .correct() function"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPAzVaVacvW0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAdLHEjNcvW4","colab_type":"code","colab":{}},"source":["# remove non-ascii character\n","def remove_non_ascii(words):\n","    import unicodedata\n","    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","        new_words.append(new_word)\n","    return new_words\n","\n","# turn digit into its word representation\n","def replace_numbers(words):\n","    import inflect\n","    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n","    p = inflect.engine()\n","    new_words = []\n","    for word in words:\n","        if word.isdigit():\n","            new_word = p.number_to_words(word)\n","            new_words.append(new_word)\n","        else:\n","            new_words.append(word)\n","    return new_words\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKqiRHoycvW_","colab_type":"text"},"source":["# Tokenization (NLTK)"]},{"cell_type":"code","metadata":{"id":"hArRDNC3cvXB","colab_type":"code","colab":{}},"source":["from nltk.tokenize import word_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY3zjUt9cvXF","colab_type":"code","colab":{}},"source":["# HINT:\n","# - just call the function, accept string / text as a parameter\n","# - [optional] test the code below to see the frequency of each word"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8jWJ4EvcvXI","colab_type":"code","colab":{}},"source":["from nltk import FreqDist\n","occurence_counter = FreqDist(tokens)\n","print(occurence_counter.most_common())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMt_kkthcvXM","colab_type":"text"},"source":["# Tokenization (SPACY)"]},{"cell_type":"code","metadata":{"id":"PY9VUc9McvXO","colab_type":"code","colab":{}},"source":["# HINT:\n","# - load the english model; nlp = en_core_web_sm.load()\n","# - pass the documents into the model\n","# - iterate over the token and access .text attribute"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BooWpZcucvXS","colab_type":"text"},"source":["# Tokenization (TextBlob)"]},{"cell_type":"code","metadata":{"id":"YHd1Fk39cvXU","colab_type":"code","colab":{}},"source":["# HINT:\n","# - access .words attribute"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIeltmXRcvXX","colab_type":"text"},"source":["# Stop Word Removal"]},{"cell_type":"code","metadata":{"id":"bCPl6kgmcvXY","colab_type":"code","colab":{}},"source":["# from nltk.corpus import stopwords\n","# stopwords.words('english')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0M1GjrncvXa","colab_type":"text"},"source":["# Stemming"]},{"cell_type":"code","metadata":{"id":"tvnXTIwjcvXb","colab_type":"code","colab":{}},"source":["from nltk.stem import PorterStemmer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wb_Niy0rcvXi","colab_type":"code","colab":{}},"source":["# HINT\n","# - create an object of PorterStemmer\n","# - [optional] vectorize function stem to avoid iterating one by one\n","# - [optional] test the LancasterStemmer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C88DJ873cvXm","colab_type":"text"},"source":["# Lemmatization"]},{"cell_type":"code","metadata":{"id":"MOAGztaGcvXn","colab_type":"code","colab":{}},"source":["from nltk.stem import WordNetLemmatizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S45jpnO-cvXq","colab_type":"code","colab":{}},"source":["# HINT\n","# - create an object of WordNetLemmatizer\n","# - [optional] vectorize function lemmatize to avoid iterating one by one"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYdzTvdMcvXv","colab_type":"text"},"source":["# Lemmatization (SPACY)"]},{"cell_type":"code","metadata":{"id":"SV216m4QcvXw","colab_type":"code","colab":{}},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hL4K45aYcvX0","colab_type":"code","colab":{}},"source":["# HINT\n","# - nlp accept a string as an input\n","# - in order to get the lemma for each word in the text, could use iteration \n","# - subsequently, access lemma_ attribute\n","# - [optional] try access .pos_ attribute"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHzjWlehcvX5","colab_type":"text"},"source":["# Part of Speech Tagging (PoSTag)"]},{"cell_type":"code","metadata":{"id":"AQqR0DJUcvX-","colab_type":"code","colab":{}},"source":["from nltk import pos_tag"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArpO8OZrcvYH","colab_type":"code","colab":{}},"source":["# HINT:\n","# - just invoke the function"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hp_c0uLecvYL","colab_type":"code","colab":{}},"source":["from textblob import TextBlob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsK0_YYXcvYO","colab_type":"code","colab":{}},"source":["# HINT\n","# - create an instance of TextBlob\n","# - access tags attributes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QUZYSsUqcvYQ","colab_type":"text"},"source":["# WordCloud"]},{"cell_type":"code","metadata":{"id":"yIWIz4rhcvYS","colab_type":"code","colab":{}},"source":["# HINT:\n","# - create an instance of WordCloud"],"execution_count":0,"outputs":[]}]}