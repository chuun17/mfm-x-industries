{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W3 - Laboratory.ipynb","version":"0.3.2","provenance":[{"file_id":"1aOsv9pS8ibhdqcTGT686zRuiZMda-mJr","timestamp":1563961227786}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"s2AhtULf9Y_p","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive\n","\n","# Code to read csv file into Colaboratory:!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive clie`nt.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CImPB7NEgWTD","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EW5hpu9PSbsP","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.metrics import auc, roc_curve, f1_score, confusion_matrix\n","from nltk.corpus import stopwords\n","stopWords = set(stopwords.words('english'))\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","np.random.seed(123)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"55iBM5iNNCJY","colab_type":"code","colab":{}},"source":["#download dataset\n","\n","id = \"1h2MguzOoQwN9Libp7YNzD-GmEV0j8jP5\"\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('offense_dataset.csv')  \n","df = pd.read_csv('offense_dataset.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8WjkO4UWjiC","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihJLxLf1UQ2R","colab_type":"code","colab":{}},"source":["train_set = df.sample(frac=0.7)\n","test_set = df[~df.index.isin(train_set.index)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oXijQTeV8lN","colab_type":"code","colab":{}},"source":["#feature vectorizer\n","count_vec = CountVectorizer(binary=True)\n","train_X = count_vec.fit_transform(train_set[\"comment_text\"])\n","\n","#label extranction\n","train_Y = train_set[\"insult\"]\n","\n","#test dataset\n","test_X = count_vec.transform(test_set[\"comment_text\"])\n","test_Y = test_set[\"insult\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fffbfa8Wnte","colab_type":"code","colab":{}},"source":["model = SGDClassifier(loss='log')\n","model.fit(train_X, train_Y)\n","\n","pred_Y = model.predict(test_X)\n","#proba_Y = model.predict_proba(test_X)[:,1]\n","#fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","#roc_auc = auc(fpr, tpr)\n","#print(roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw5OVXpnaA18","colab_type":"code","colab":{}},"source":["model = LogisticRegression()\n","model.fit(train_X, train_Y)\n","pred_Y = model.predict(test_X)\n","proba_Y = model.predict_proba(test_X)[:,1]\n","fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","roc_auc = auc(fpr, tpr)\n","print(\"auc\", roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wwGQGPHmamLL","colab_type":"text"},"source":["# Removal of Stopwords\n","\n","In this section we demonstrate the effect of removing stopwords from the feature space\n"]},{"cell_type":"code","metadata":{"id":"1ovx6o7ygb_X","colab_type":"code","colab":{}},"source":["def remove_stopword(sentence):\n","  return \" \".join([x.strip(\".',!?-<>\") for x in sentence.split() if x not in stopWords])\n","\n","#feature vectorizer\n","count_vec = CountVectorizer(binary=True)\n","train_X = count_vec.fit_transform(train_set[\"comment_text\"].apply(remove_stopword))\n","\n","#label extranction\n","train_Y = train_set[\"insult\"]\n","\n","#test dataset\n","test_X = count_vec.transform(test_set[\"comment_text\"].apply(remove_stopword))\n","test_Y = test_set[\"insult\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxfy5fkPg7yq","colab_type":"code","colab":{}},"source":["print(\"SGD Classifier\")\n","model = SGDClassifier(loss='log')\n","model.fit(train_X, train_Y)\n","\n","pred_Y = model.predict(test_X)\n","#proba_Y = model.predict_proba(test_X)[:,1]\n","#fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","#roc_auc = auc(fpr, tpr)\n","#print(roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))\n","\n","\n","print(\"Logistic Regression\")\n","model = LogisticRegression()\n","model.fit(train_X, train_Y)\n","pred_Y = model.predict(test_X)\n","proba_Y = model.predict_proba(test_X)[:,1]\n","fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","roc_auc = auc(fpr, tpr)\n","print(\"auc\", roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYuyOXr1foau","colab_type":"text"},"source":["# Changing the Vectorizer to Tf-Idf"]},{"cell_type":"code","metadata":{"id":"AgWjqIUodg88","colab_type":"code","colab":{}},"source":["def remove_stopword(sentence):\n","  return \" \".join([x.strip(\".',!?-<>\") for x in sentence.split() if x not in stopWords])\n","\n","#feature vectorizer\n","tfidf_vec = TfidfVectorizer(binary=True,\n","                            smooth_idf=True,\n","                            use_idf = True,\n","                            sublinear_tf = False\n","                           )\n","train_X = tfidf_vec.fit_transform(train_set[\"comment_text\"])\n","\n","#label extranction\n","train_Y = train_set[\"insult\"]\n","\n","#test dataset\n","test_X = tfidf_vec.transform(test_set[\"comment_text\"])\n","test_Y = test_set[\"insult\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-2Msbgff0q9","colab_type":"code","colab":{}},"source":["print(\"SGD Classifier\")\n","model = SGDClassifier(loss='log')\n","model.fit(train_X, train_Y)\n","\n","pred_Y = model.predict(test_X)\n","#proba_Y = model.predict_proba(test_X)[:,1]\n","#fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","#roc_auc = auc(fpr, tpr)\n","#print(roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))\n","\n","\n","print(\"Logistic Regression\")\n","model = LogisticRegression(penalty='l2', class_weight='balanced')\n","model.fit(train_X, train_Y)\n","pred_Y = model.predict(test_X)\n","proba_Y = model.predict_proba(test_X)[:,1]\n","fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","roc_auc = auc(fpr, tpr)\n","print(\"auc\", roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_G0n_8P0iczC","colab_type":"text"},"source":["# Trying Out Neural Network"]},{"cell_type":"code","metadata":{"id":"IO--BCBTHR9v","colab_type":"code","colab":{}},"source":["from keras import backend as K\n","\n","def f1(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        \"\"\"Recall metric.\n","\n","        Only computes a batch-wise average of recall.\n","\n","        Computes the recall, a metric for multi-label classification of\n","        how many relevant items are selected.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","    def precision(y_true, y_pred):\n","        \"\"\"Precision metric.\n","\n","        Only computes a batch-wise average of precision.\n","\n","        Computes the precision, a metric for multi-label classification of\n","        how many selected items are relevant.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuc11WE8f9Tu","colab_type":"code","colab":{}},"source":["def remove_stopword(sentence):\n","  return \" \".join([x.strip(\".',!?-<>\") for x in sentence.split() if x not in stopWords])\n","\n","# #feature vectorizer\n","# count_vec = TfidfVectorizer(lowercase=True,\n","#                             max_features=5000,\n","#                             norm=None\n","#                            )\n","\n","count_vec = CountVectorizer(binary=True)\n","train_X = count_vec.fit_transform(train_set[\"comment_text\"].apply(remove_stopword))\n","\n","#label extranction\n","train_Y = train_set[\"insult\"]\n","\n","#test dataset\n","test_X = count_vec.transform(test_set[\"comment_text\"].apply(remove_stopword))\n","test_Y = test_set[\"insult\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7Dq0hYiisau","colab_type":"code","colab":{}},"source":["model = Sequential()\n","input_dim = test_X.shape[1]\n","model.add(Dense(512, input_dim=input_dim, activation='relu'))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvdnGKdqrrjk","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Sp03aIOsjBov","colab_type":"code","colab":{}},"source":["model.fit(train_X, train_Y, epochs=20, batch_size = 2000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHRWww6Rrr2s","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"cuEToC_s9SOk","colab_type":"code","colab":{}},"source":["pred_Y = [round(x[0]) for x in model.predict(test_X)]\n","#proba_Y = model.predict_proba(test_X)[:,1]\n","#fpr, tpr, thresholds = roc_curve(test_Y, proba_Y)\n","#roc_auc = auc(fpr, tpr)\n","#print(\"auc\", roc_auc)\n","print(\"f1 score\", f1_score(test_Y, pred_Y))\n","print(\"conf. matrix\", confusion_matrix(test_Y, pred_Y).ravel())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTHJQ8OJqdQF","colab_type":"text"},"source":["# Inserting Embeddings to our Model"]},{"cell_type":"code","metadata":{"id":"8NT03a7EZ5ek","colab_type":"code","colab":{}},"source":["#download dataset\n","\n","id = \"1XeMRO9CTTqcSLRQK17mm6GRum1-ZSdb6\"\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('glove.6B.50d.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxkmqAMpscl6","colab_type":"code","colab":{}},"source":["word_vectors = {}\n","with open('glove.6B.50d.txt') as f:\n","    for line in f.read().splitlines():\n","        tokens = line.split()\n","        word = tokens[0]\n","        vector = tokens[1:]\n","        word_vectors[word] = vector"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cyqr8T1KtMk3","colab_type":"text"},"source":["## Build our vocab from training data"]},{"cell_type":"code","metadata":{"id":"8a4RDXH7sesF","colab_type":"code","colab":{}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Flatten, Lambda, Dropout\n","from keras.layers import LSTM\n","import keras.backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsJoiBBitDPA","colab_type":"code","colab":{}},"source":["x_train = train_set['comment_text'].values\n","y_train = train_set[['identity_hate', 'insult', 'obscene']].astype(int).values\n","\n","x_test = test_set['comment_text'].values\n","y_test = test_set[['identity_hate', 'insult', 'obscene']].astype(int).values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bqU1vLxtZ6a","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(num_words=25000, oov_token='<unk>')\n","tokenizer.fit_on_texts(x_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Of5CmLS6tg7i","colab_type":"code","colab":{}},"source":["vocab = [w for i, w in tokenizer.index_word.items() if i <= 25000]\n","wordToIndex = {w: i+1 for i, w in enumerate(vocab)}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJZcv_kbxj0f","colab_type":"text"},"source":["## Find the best maxlen for our input"]},{"cell_type":"code","metadata":{"id":"NfFlli0Uxg7q","colab_type":"code","colab":{}},"source":["train_seq = tokenizer.texts_to_sequences(x_train)\n","sent_length = [len(s) for s in train_seq]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGjGmpwQx182","colab_type":"code","colab":{}},"source":["plt.boxplot(sent_length, vert=False)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvMC_iaSx_T_","colab_type":"code","colab":{}},"source":["plt.hist(sent_length, bins=100)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmk-y8bhyDmm","colab_type":"code","colab":{}},"source":["maxlen = 200"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwJ79bXZt-xw","colab_type":"text"},"source":["## Prepare the embedding matrix"]},{"cell_type":"code","metadata":{"id":"qiiGJ7eltowM","colab_type":"code","colab":{}},"source":["embedding_matrix = np.zeros((len(wordToIndex)+2, 50))\n","\n","for word, idx in wordToIndex.items():\n","    if word in word_vectors:\n","        embedding_matrix[idx] = word_vectors[word]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-KdcZqMweLz","colab_type":"code","colab":{}},"source":["embedding_matrix.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaudXBVWxQJ-","colab_type":"code","colab":{}},"source":["n_vocab, embed_dim = embedding_matrix.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbp5j_Whw_r3","colab_type":"text"},"source":["## Let's put the pre-trained embedding matrix in our model"]},{"cell_type":"code","metadata":{"id":"t1yiXNHCt6Kp","colab_type":"code","colab":{}},"source":["# Deep Averaging Network\n","model = Sequential()\n","model.add(Embedding(input_dim=n_vocab, output_dim=embed_dim, input_length=maxlen, weights=[embedding_matrix]))\n","model.add(Lambda(lambda x: K.mean(x, axis=1)))\n","model.add(Dense(512))\n","model.add(Dense(512))\n","model.add(Dense(3, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', metrics=['acc', f1], optimizer='adam')\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqcaf9gqyLum","colab_type":"text"},"source":["## Transfrom the dataset into sequence of integers instead of list of string"]},{"cell_type":"code","metadata":{"id":"tjyssfvMyK6G","colab_type":"code","colab":{}},"source":["x_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=maxlen)\n","x_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=maxlen)\n","print('x_train: ', x_train.shape)\n","print('x_test:', x_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmjPCucpynLd","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"ZonEUFT4uJKg","colab_type":"code","colab":{}},"source":["model.fit(x_train, y_train, epochs=5, batch_size=128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGlvg5OFyy4H","colab_type":"text"},"source":["## Test the model"]},{"cell_type":"code","metadata":{"id":"JfkyEKjXu_MC","colab_type":"code","colab":{}},"source":["loss, ac, f = model.evaluate(x_test, y_test, batch_size=128)\n","print('Loss: ', loss)\n","print('Accuracy: ', ac)\n","print('F1: ', f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8OAPEz7k1eKq","colab_type":"text"},"source":["# Multi-Class Classification"]},{"cell_type":"code","metadata":{"id":"HifGyzFc8bUO","colab_type":"code","colab":{}},"source":["mapping ={\n","    (0,0,0):0,\n","    (0,0,1):1,\n","    (0,1,0):2,\n","    (0,1,1):3,\n","    (1,0,0):4,\n","    (1,0,1):5,\n","    (1,1,0):6,\n","    (1,1,1):7\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1eEW92OxsYh","colab_type":"code","colab":{}},"source":["#feature vectorizer\n","count_vec = CountVectorizer(binary=True)\n","train_X = count_vec.fit_transform(train_set[\"comment_text\"])\n","\n","#label extranction\n","train_Y1 = train_set[\"insult\"]\n","train_Y2 = train_set[\"obscene\"]\n","train_Y3 = train_set[\"identity_hate\"]\n","\n","#test dataset\n","test_X = count_vec.transform(test_set[\"comment_text\"])\n","test_Y1 = test_set[\"insult\"]\n","test_Y2 = train_set[\"obscene\"]\n","test_Y3 = train_set[\"identity_hate\"]\n","test_Y = [mapping[x] for x in zip(test_Y1, test_Y2, test_Y3)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"85h3tQiW2xdq","colab_type":"code","colab":{}},"source":["model_1 = SGDClassifier(loss='log')\n","model_2 = SGDClassifier(loss='log')\n","model_3 = SGDClassifier(loss='log')\n","\n","model_1.fit(train_X, train_Y1)\n","model_2.fit(train_X, train_Y2)\n","model_3.fit(train_X, train_Y3)\n","\n","pred_Y1 = model_1.predict(test_X)\n","pred_Y2 = model_2.predict(test_X)\n","pred_Y3 = model_3.predict(test_X)\n","\n","pred_Y = [mapping[x] for x in zip(pred_Y1, pred_Y2, pred_Y3)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBxMRGjI56cy","colab_type":"code","colab":{}},"source":["print(\"f1 score\", f1_score(test_Y, pred_Y, average='macro'))\n","print(\"f1 score\", f1_score(test_Y, pred_Y, average='micro'))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLtHnVhD6mQ6","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","print(classification_report(test_Y, pred_Y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8_E8TPg9-OC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}